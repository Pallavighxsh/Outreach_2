# Outreach_2 

Building on my earlier CLI tool Outreach, this enhanced version goes beyond just collecting names and emails.  

While the previous release is useful if you only need basic contact details, this updated version provides a more comprehensive scrape with richer information.  

A Selenium + BeautifulSoup powered crawler for discovering and scraping contact information (emails, phone numbers, names, and roles) from organizational websites.  

DISCLAIMER: DO NOT USE THIS FOR THE PURPOSE OF SPAMMING.  

THE INTENTION OF THIS TOOL IS TO HELP DIGITAL MARKETERS.  

SPAMMERS, BE GONE!  

---

üöÄ What It Does  

- Starts from one or more seed websites.  

- Crawls outward to discover new pages.  

- Collects links that look like contact directories or team listings (for example, pages with ‚Äúcontact‚Äù, ‚Äúteam‚Äù, ‚Äúpeople‚Äù, ‚Äústaff‚Äù, ‚Äúdirectory‚Äù, or ‚Äúabout-us‚Äù in the URL).  

- Extracts details such as:  

  - Email addresses  

  - Phone numbers  

  - Names (where available)  

  - Roles or designations (e.g., Manager, Director, Engineer, Faculty)  

  - Saves results into a clean Excel file for easy use.  

- Includes resume functionality, so you can safely stop and restart the process without losing progress.  

---

üõ†Ô∏è Setup  

It is strongly recommended to use a Python virtual environment when running this project.  

This keeps dependencies isolated and avoids version conflicts across projects.  

Once the environment is created and activated, install the dependencies:  

---

üìã Requirements  

Before running the scraper, make sure you have the following set up:  

- Python 3.9+ (tested up to Python 3.13)  

- Google Chrome installed on your system  

- Virtual environment (recommended to keep dependencies isolated)  

Python libraries needed:  

- selenium ‚Äì for automated browsing  

- webdriver-manager ‚Äì to automatically download the correct ChromeDriver version  

- beautifulsoup4 ‚Äì for parsing page content  

- openpyxl ‚Äì for saving results to Excel  

- requests ‚Äì for handling web requests  

- pandas (optional, but useful if you want to do post-processing)  

---

‚öôÔ∏è How to Run  

- Define the seed websites you want to start crawling from inside the script. (SEED_URLS =)  

- Install the required packages.  

- Start the program with Python.  

---

‚è∏Ô∏è Resume Functionality (Highly Recommended)  

This scraper saves its progress to a file.  

If you stop the program with Ctrl+C, it will store the list of pages already visited and any contacts found so far.  

When you restart, the crawler will resume from the saved state, instead of starting over.  

This feature is not just convenient ‚Äî it is recommended for safety.  

Sending too many requests to a site in one go can trigger rate-limiting or even temporary IP blocking.  

By using the resume functionality, you can:  

- Break long crawls into smaller sessions.  

- Spread requests out over time. You can even get the program to sleep or wait between requests for this purpose.  

- It protects your IP address from getting blocked by the server.  

- Avoid overwhelming websites with too many requests and attracting suspicion.  

In practice:  

- Run the scraper for a while.  

- Stop it safely with Ctrl+C.  

- Resume later, picking up where you left off.  

---

üìÇ Output  

All scraped contacts are stored in an Excel spreadsheet.  

Each row includes:  

- The page URL where contact info was found.  

- Emails detected.  

- Phone numbers detected.  

- Names found (where identifiable).  

- Roles or designations mentioned.  

---

‚ö†Ô∏è Notes  

- Always run inside a virtual environment.  

- Chrome runs in headless mode by default, so you won‚Äôt see a browser window.  

  Comment out the headless mode if you want to see your screen.  

  Sometimes, this helps Selenium work better(??).  

- The crawler includes random short delays between requests, but you should still use the resume functionality to avoid flooding websites.  

- If Chrome updates on your machine, the correct driver is automatically downloaded.  
