import os
import re
import time
import random
import pickle
from collections import deque
from urllib.parse import urlparse, urljoin

from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.common.exceptions import TimeoutException, WebDriverException
from webdriver_manager.chrome import ChromeDriverManager

import openpyxl

# -------------------------
# NLTK SETUP
# -------------------------
import nltk
from nltk import word_tokenize, pos_tag
from nltk.corpus import stopwords

nltk.download("punkt")
nltk.download("averaged_perceptron_tagger")
nltk.download("stopwords")

STOPWORDS = set(stopwords.words("english"))

# -------------------------
# CONFIG
# -------------------------
OUTPUT_FILE = "contacts.xlsx"
STATE_FILE = "scraper_state.pkl"

MAX_PAGES = 2000
PAGE_LOAD_TIMEOUT = 7
RANDOM_DELAY_RANGE = (1.0, 3.5)

URL_KEYWORDS = ["contact", "team", "people", "staff", "directory", "about-us"]

SEED_URLS = [
    "https://www.example.com",
]

# -------------------------
# Helper Functions
# -------------------------

def random_sleep():
    """Human-like jitter."""
    time.sleep(random.uniform(*RANDOM_DELAY_RANGE))


def normalize_url(base_url, link):
    return urljoin(base_url, link)


def is_valid_url(url):
    parsed = urlparse(url)
    return parsed.scheme in ("http", "https")


# -------------------------
# NLTK EXTRACTION LOGIC
# -------------------------

def extract_proper_nouns(body_text):
    """Extract ALL proper nouns (NNP, NNPS)."""
    try:
        tokens = word_tokenize(body_text)
        tagged = pos_tag(tokens)
        return [word for word, tag in tagged if tag in ("NNP", "NNPS")]
    except Exception:
        return []


def extract_names_nltk(body_text):
    """Extract two-word sequences: NNP NNP (names)."""
    try:
        tokens = word_tokenize(body_text)
        tagged = pos_tag(tokens)
        names = []
        for i in range(len(tagged) - 1):
            if tagged[i][1] == "NNP" and tagged[i+1][1] == "NNP":
                names.append(f"{tagged[i][0]} {tagged[i+1][0]}")
        return list(set(names))
    except Exception:
        return []


def extract_buzzwords(proper_nouns, names):
    """Buzzwords = all proper nouns except names and stopwords."""
    
    # Split names into individual words to avoid counting them
    name_parts = set()
    for name in names:
        for part in name.split():
            name_parts.add(part)

    buzz = []
    for word in proper_nouns:

        # Remove name components
        if word in name_parts:
            continue

        # Skip trivial or lowercase or meaningless capital words
        if word.lower() in STOPWORDS:
            continue
        if len(word) <= 2:
            continue
        if not any(c.isalpha() for c in word):
            continue

        buzz.append(word)

    return list(set(buzz))


def extract_contact_info(body_text):
    """Emails + NLTK Names + NLTK Buzzwords."""
    data = {}

    # Emails
    emails = re.findall(
        r"[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}",
        body_text
    )
    data["emails"] = list(set(emails))

    # Names
    names = extract_names_nltk(body_text)
    data["names"] = names

    # Buzzwords derived from proper nouns
    proper_nouns = extract_proper_nouns(body_text)
    buzzwords = extract_buzzwords(proper_nouns, names)
    data["buzzwords"] = buzzwords

    return data


# -------------------------
# MAIN SCRAPER LOGIC
# -------------------------

def scrape_contacts():

    # Setup Selenium
    options = Options()
    options.add_argument("--headless=new")
    options.add_argument("--disable-gpu")
    options.add_argument("--no-sandbox")

    driver = webdriver.Chrome(
        service=Service(ChromeDriverManager().install()),
        options=options
    )
    driver.set_page_load_timeout(PAGE_LOAD_TIMEOUT)

    # Resume if exists
    if os.path.exists(STATE_FILE):
        print("[INFO] Resuming previous session...")
        with open(STATE_FILE, "rb") as f:
            to_visit, visited, contact_pages, results = pickle.load(f)
    else:
        print("[INFO] Starting fresh crawl...")
        to_visit = deque(SEED_URLS)
        visited = set()
        contact_pages = set()
        results = []

    try:
        while to_visit and len(visited) < MAX_PAGES:

            url = to_visit.popleft()
            if url in visited:
                continue

            print(f"\n➡️  Visiting: {url}")
            visited.add(url)

            # Load page with timeout
            try:
                driver.get(url)
            except TimeoutException:
                print("   ⚠️ Timeout — skipping")
                continue
            except WebDriverException as e:
                print(f"   ⚠️ WebDriver error: {e}")
                continue

            random_sleep()

            # Parse via BeautifulSoup
            soup = BeautifulSoup(driver.page_source, "html.parser")
            body_text = soup.get_text(separator=" ", strip=True)

            # Identify contact-like pages
            if any(k in url.lower() for k in URL_KEYWORDS):
                info = extract_contact_info(body_text)
                results.append({"url": url, **info})
                contact_pages.add(url)
                print(f"   ✔ Contact-like page found (emails={len(info['emails'])}, names={len(info['names'])})")

            # Queue internal links
            for tag in soup.find_all("a", href=True):
                link = normalize_url(url, tag["href"])

                if not is_valid_url(link):
                    continue
                if urlparse(link).netloc != urlparse(url).netloc:
                    continue
                if link not in visited:
                    to_visit.append(link)

            # Progress update
            if len(visited) % 25 == 0:
                print(f"[PROGRESS] Visited {len(visited)} pages | Contact pages found: {len(contact_pages)}")

    except KeyboardInterrupt:
        print("\n[INFO] Manual interrupt — saving progress...")

    finally:
        driver.quit()

        # Save state for future resume
        with open(STATE_FILE, "wb") as f:
            pickle.dump((to_visit, visited, contact_pages, results), f)

        # Save results to Excel
        if results:
            wb = openpyxl.Workbook()
            ws = wb.active
            ws.append(["URL", "Emails", "Names", "Buzzwords"])

            for r in results:
                ws.append([
                    r.get("url"),
                    ", ".join(r.get("emails", [])),
                    ", ".join(r.get("names", [])),
                    ", ".join(r.get("buzzwords", []))
                ])

            wb.save(OUTPUT_FILE)
            print(f"[INFO] Saved {len(results)} records → {OUTPUT_FILE}")
        else:
            print("[INFO] No records found.")

if __name__ == "__main__":
    scrape_contacts()
