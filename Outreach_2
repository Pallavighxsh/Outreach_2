"""

Key features:
- Starts from seed URLs and crawls subpages
- Collects links containing contact-related keywords
- Scrapes emails, phone numbers, names, designations -- check the README.md file for how to customize
- Saves results to Excel
- Supports safe resume after Ctrl+C
"""

import os
import re
import time
import random
import pickle
from collections import deque
from urllib.parse import urlparse, urljoin

from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from webdriver_manager.chrome import ChromeDriverManager
import openpyxl

# --- CONFIG ---
OUTPUT_FILE = "contacts.xlsx"
STATE_FILE = "scraper_state.pkl"

# Contact-related keywords for URL filtering (can be customized per industry)
URL_KEYWORDS = ["contact", "team", "people", "staff", "directory", "about-us"]

# Seed URLs (edit as needed)
SEED_URLS = [
    "https://www.example.com",
]

# --- Helper functions ---
def normalize_url(base_url, link):
    return urljoin(base_url, link)

def is_valid_url(url):
    parsed = urlparse(url)
    return parsed.scheme in ("http", "https")

def extract_contact_info(body_text):
    """Regex-based extraction of key contact fields from page text"""
    data = {}

    # Emails
    email_match = re.findall(r"[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}", body_text)
    data["emails"] = list(set(email_match))

    # Phone numbers (basic international formats)
    phone_match = re.findall(r"\+?\d[\d\s\-]{7,}\d", body_text)
    data["phones"] = list(set(phone_match))

    # Names (very rough heuristic – capitalized words, tweak if needed)
    name_match = re.findall(r"\b[A-Z][a-z]+ [A-Z][a-z]+\b", body_text)
    data["names"] = list(set(name_match))

    # Designations / roles (simple keyword matching)
    roles = []
    for role in ["Manager", "Director", "Executive", "CTO", "Professor", "Coordinator", "Engineer"]:
        if role.lower() in body_text.lower():
            roles.append(role)
    data["roles"] = list(set(roles))

    return data

# --- Main scraper ---
def scrape_contacts():
    # Selenium setup
    options = webdriver.ChromeOptions()
    options.add_argument("--headless=new")
    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)

    # Resume or start fresh
    if os.path.exists(STATE_FILE):
        print("[INFO] Resuming crawl from saved state...")
        with open(STATE_FILE, "rb") as f:
            state = pickle.load(f)
        to_visit, visited, contact_pages, results = state
    else:
        print("[INFO] Starting fresh crawl...")
        to_visit = deque(SEED_URLS)
        visited = set()
        contact_pages = set()
        results = []

    try:
        while to_visit:
            url = to_visit.popleft()
            if url in visited:
                continue
            visited.add(url)

            try:
                driver.get(url)
                time.sleep(random.uniform(1, 3))  # polite delay
                soup = BeautifulSoup(driver.page_source, "html.parser")
                body_text = soup.get_text(separator=" ", strip=True)

                # Check if URL looks like a "contact/team/staff" page
                if any(keyword in url.lower() for keyword in URL_KEYWORDS):
                    info = extract_contact_info(body_text)
                    results.append({"url": url, **info})
                    contact_pages.add(url)

                # Enqueue new links
                for link_tag in soup.find_all("a", href=True):
                    link = normalize_url(url, link_tag["href"])
                    if is_valid_url(link) and urlparse(link).netloc == urlparse(url).netloc:
                        if link not in visited:
                            to_visit.append(link)

                if len(visited) % 25 == 0:
                    print(f"Visited {len(visited)} pages — found {len(contact_pages)} contact pages...")

            except Exception as e:
                print(f"[WARN] Failed to process {url}: {e}")

    except KeyboardInterrupt:
        print("\n[INFO] Interrupted by user — saving state and partial results...")

    finally:
        driver.quit()

        # Save state for resume
        with open(STATE_FILE, "wb") as f:
            pickle.dump((to_visit, visited, contact_pages, results), f)

        # Save results to Excel
        if results:
            wb = openpyxl.Workbook()
            ws = wb.active
            ws.append(["URL", "Emails", "Phones", "Names", "Roles"])
            for r in results:
                ws.append([
                    r.get("url"),
                    ", ".join(r.get("emails", [])),
                    ", ".join(r.get("phones", [])),
                    ", ".join(r.get("names", [])),
                    ", ".join(r.get("roles", []))
                ])
            wb.save(OUTPUT_FILE)
            print(f"[INFO] Saved {len(results)} rows to {OUTPUT_FILE}")
        else:
            print("[INFO] No results found to save.")

if __name__ == "__main__":
    scrape_contacts()
